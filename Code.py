# -*- coding: utf-8 -*-
"""KMeans_Final

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z46tVS6lAIJAAJwSjnT3s64Ug0cERrb5
"""

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/MyDrive/Colab Notebooks/Data.gpkg'

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

!pip install geopandas

import geopandas

pip install libpysal

!pip install mapclassify

import mapclassify

# Read file
data = geopandas.read_file(path)

data.columns = data.columns.to_series().apply(lambda x: x.strip())

cluster_variables = ["Acc_Sum", "SIP", "BUA2020", "ChangeBUA_Fourth", "Block_Area" ]
#cluster_variables = ["Acc_Log", "SIP_Fourth", "LPN_Fourth", "BUA2020", "ChangeBUA_Fourth", "BlockArea_Fourth"]

data[cluster_variables]

f, axs = plt.subplots(nrows=2, ncols=3, figsize=(20, 20))
# Make the axes accessible with single indexing
axs = axs.flatten()
# Start a loop over all the variables of interest
for i, col in enumerate(cluster_variables):
    # select the axis where the map will go
    ax = axs[i]
    # Plot the map
    data.plot(
        column=col,
        ax=ax,
        scheme="Quantiles",
        linewidth=0,
        cmap="RdPu",
    )
    # Remove axis clutter
    ax.set_axis_off()
    # Set the axis title to the name of variable being plotted
    ax.set_title(col)
# Display the figure
plt.show()

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data[cluster_variables])
data_scaled

#Kmeans
from sklearn.cluster import KMeans
# Initialise KMeans instance
#Find optimum clusters
wcss=[]
for i in range(2,10):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=500, n_init=50, random_state=0)
    kmeans.fit(data_scaled)
    wcss_iter = kmeans.inertia_
    wcss.append(wcss_iter)

number_clusters = range(2,10)
    plt.plot(number_clusters,wcss)
    plt.title('The Elbow Tran_MinMaXScale(var=5)')
    plt.xlabel('Number of clusters')
    plt.ylabel('WCSS')

from sklearn.metrics import silhouette_score, davies_bouldin_score,v_measure_score

km_scores= []
km_silhouette = []
vmeasure_score =[]
db_score = []
for i in range(2,10):
    km = KMeans(n_clusters=i, random_state=0).fit(data_scaled)
    preds = km.predict(data_scaled)
    
    print("Score for number of cluster(s) {}: {}".format(i,km.score(data_scaled)))
    km_scores.append(-km.score(data_scaled))
    
    silhouette = silhouette_score(data_scaled, preds)
    km_silhouette.append(silhouette)
    print("Silhouette score for number of cluster(s) {}: {}".format(i,silhouette))
    
    db = davies_bouldin_score(data_scaled,preds)
    db_score.append(db)
    print("Davies Bouldin score for number of cluster(s) {}: {}".format(i,db))

plt.figure(figsize=(7,4))
plt.title("The silhouette coefficient method \nfor determining number of clusters\n",fontsize=16)
plt.scatter(x=[i for i in range(2,10)],y=km_silhouette,s=150,edgecolor='k')
plt.grid(True)
plt.xlabel("Number of clusters",fontsize=14)
plt.ylabel("Silhouette score",fontsize=15)
plt.xticks([i for i in range(2,12)],fontsize=14)
plt.yticks(fontsize=15)
plt.show()

plt.scatter(x=[i for i in range(2,10)],y=db_score,s=150,edgecolor='k')
plt.grid(True)
plt.xlabel("Davies-Bouldin score")
plt.show()

# Initialise KMeans instance
from sklearn.cluster import KMeans

# Initialise KMeans instance
kmeans = KMeans(n_clusters=6)

# Run K-Means algorithm
k5cls = kmeans.fit(data_scaled)

k5cls.labels_[:6]

# Assign labels into a column
data["k5cls"] = k5cls.labels_
# Setup figure and ax
f, ax = plt.subplots(1, figsize=(9, 9))
# Plot unique values choropleth including
# a legend and with no boundary lines
data.plot(
    column="k5cls", categorical=True, legend=True, linewidth=0, ax=ax
)
# Remove axis
ax.set_axis_off()
# Display the map
plt.show()

data

# Group data table by cluster label and count observations
k5sizes = data.groupby("k5cls").size()
k5sizes

# Group table by cluster label, keep the variables used
# for clustering, and obtain their mean
k5means = data.groupby("k5cls")[cluster_variables].mean()
# Transpose the table and print it rounding each value
# to three decimals
k5means.T.round(3)

# Index db on cluster ID
tidy_data = data.set_index("k5cls")
# Keep only variables used for clustering
tidy_data = tidy_data[cluster_variables]
# Stack column names into a column, obtaining
# a "long" version of the dataset
tidy_data = tidy_data.stack()
# Take indices into proper columns
tidy_data = tidy_data.reset_index()
# Rename column names
tidy_data = tidy_data.rename(
    columns={"level_1": "Attribute", 0: "Values"}
)
# Check out result
tidy_data.head()

import seaborn

# Scale fonts to make them more readable
seaborn.set(font_scale=1.5)
# Setup the facets
facets = seaborn.FacetGrid(
    data=tidy_data,
    col="Attribute",
    hue="k5cls",
    sharey=False,
    sharex=False,
    aspect=2,
    col_wrap=3,
)
# Build the plot from `sns.kdeplot`
_ = facets.map(seaborn.kdeplot, "Values", shade=True).add_legend()

# Fit algorithm to the data
#clusters = kmeans.fit_predict(data_scaled)

#clusters = pd.DataFrame(clusters)
#clusters.columns = ['clusID']
#clusters

# determining the name of the file
file_name = 'k5meansTranMinMaxClean2(v=5).xlsx'
  # saving the excel
data["k5cls"].to_excel(file_name)
print('DataFrame is written to Excel File successfully.')

!pip install shap

import shap

import lightgbm as lgb

params = lgb.LGBMClassifier().get_params()
params

params['objective'] = 'multiclass' # the target to predict is the number of the cluster
params['is_unbalance'] = True
params['n_jobs'] = -1
params['random_state'] = 1301

mdl = lgb.LGBMClassifier(**params)

X =  data[cluster_variables]

X

mdl.fit(X, data["k5cls"])

y_pred = mdl.predict_proba(X)

explainer = shap.TreeExplainer(mdl)
shap_values = explainer.shap_values(X)

shap.summary_plot(shap_values, X, max_display=30)

for cnr in data["k5cls"].unique():
    shap.summary_plot(shap_values[cnr], X, max_display=30, show=False)
    plt.title(f'Cluster {cnr}')
    plt.show()

cnr = 0
shap.summary_plot(shap_values[cnr], X, max_display=30, show=False)
plt.title(f'Cluster {cnr}')
plt.show()

from pandas.plotting import parallel_coordinates
from matplotlib.collections import LineCollection

data_scaled

data_new = data[cluster_variables]
data_new = pd.DataFrame(data_scaled, index= data_new.index, columns = data_new.columns)
data_new

data_["cluster"]= data["k5cls"]
data_cluster

pd.plotting.parallel_coordinates(data_new, 'cluster')

parallel_coordinates(data_new, class_column='cluster', cols=['Acc_Sum', 'SIP', 'BUA2020', 'ChangeBUA_Fourth', 'Block_Area'])

centroids = pd.DataFrame(kmeans.cluster_centers_, columns = data_new.columns)

centroids['cluster'] = centroids.index

centroids

pd.plotting.parallel_coordinates(centroids, 'cluster', color=['g', 'm', 'r', 'b', 'y', 'lime'])